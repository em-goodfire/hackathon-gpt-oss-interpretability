{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d18475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.14.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch)\n",
      "  Downloading triton-3.4.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.4.0->torch) (59.6.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Downloading torch-2.8.0-cp310-cp310-manylinux_2_28_x86_64.whl (888.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.0/888.0 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m136.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m130.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m141.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m140.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m144.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m142.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.4.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 MB\u001b[0m \u001b[31m149.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/23\u001b[0m [torch]m22/23\u001b[0m [torch]-cusolver-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.7.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.8.0 triton-3.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /mnt/polished-lake/home/mark/.local/lib/python3.10/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/polished-lake/home/mark/.local/lib/python3.10/site-packages (from huggingface_hub) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (25.0)\n",
      "Collecting pyyaml>=5.1 (from huggingface_hub)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.4)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.14.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2025.7.9)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, pyyaml, hf-xet, huggingface_hub\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [huggingface_hub] [huggingface_hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.7 huggingface_hub-0.34.4 pyyaml-6.0.2 tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a292c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae import load_sae\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=\"Goodfire/Hackathon-gpt-oss-20b-SAE-l15\",\n",
    "    filename=\"topk_sae_l15_exp16_k32.pt\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sae = load_sae(file_path, device, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc732404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopKSAE(\n",
       "  (encoder_linear): Linear(in_features=2880, out_features=46080, bias=False)\n",
       "  (decoder): Embedding(46080, 2880)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a360d8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([[4.2541, 4.1714, 3.8353, 3.5973, 3.5724, 3.5609, 3.4909, 3.4741, 3.4395,\n",
      "         3.4331, 3.4131, 3.4072, 3.3891, 3.3821, 3.3126, 3.3058, 3.2795, 3.2595,\n",
      "         3.2311, 3.2204, 3.2190, 3.1510, 3.1376, 3.1132, 3.1037, 3.0554, 3.0333,\n",
      "         3.0318, 3.0195, 3.0117, 3.0117, 2.9994]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[28863, 18118,  8643, 18829, 32025, 30785, 24263, 15114, 28406, 14496,\n",
      "         20061,  5210, 34951, 35550, 12878, 14339, 32027, 42351, 17683, 18138,\n",
      "         37700,  4499, 34537, 21550, 18191,  9191, 38523, 16228, 11765, 14591,\n",
      "         30525, 27080]], device='cuda:0'))\n",
      "torch.Size([1, 2880])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 2880).to(device)\n",
    "f = sae.encode(x)\n",
    "print(f)\n",
    "\n",
    "x_hat = sae.decode(f)\n",
    "print(x_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cbf93a",
   "metadata": {},
   "source": [
    "# Using with gpt-oss 20b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0126faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/polished-lake/home/mark/projects/harmony/.venv/lib/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3a3252dcf844dab4dde2d3ccf65c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fcbf527e674363bbdebbb7a7020b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71b4b23c5de046268c5701c0cb449321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77cbb6599fd4ad1b4a1886b2d814609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name =\"openai/gpt-oss-20b\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7bc1d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 Life: [(31259, 15505.759), (40696, 9913.289), (46030, 9502.822), (31435, 7052.571), (24613, 6508.846), (11636, 6375.023), (5005, 6370.8), (21550, 6172.671), (26632, 6044.3), (25192, 5627.713), (37743, 5416.535), (16444, 5147.6), (23035, 5097.8), (26550, 4979.968), (2560, 4919.771), (3813, 4912.863), (44863, 4894.991), (37970, 4835.599), (3725, 4807.5), (12883, 4713.53), (2512, 4691.869), (7045, 4686.243), (20137, 4683.039), (38371, 4621.383), (12531, 4611.111), (13284, 4561.846), (14634, 4561.51), (43669, 4551.993), (14926, 4486.74), (10362, 4437.937), (43273, 4290.663), (25254, 4270.169)]\n",
      "01 Ġis: [(6991, 2341.818), (4576, 814.181), (37972, 432.386), (18118, 427.344), (46030, 406.089), (21909, 305.816), (34689, 269.797), (10856, 262.572), (1687, 259.726), (28731, 255.91), (24657, 240.407), (31259, 230.828), (14679, 218.373), (17495, 189.278), (21952, 188.938), (20557, 188.557), (37880, 182.809), (34312, 179.551), (2901, 176.749), (37700, 171.254), (9545, 168.087), (20111, 164.419), (2000, 162.759), (1597, 149.029), (21557, 139.717), (26034, 136.946), (39215, 134.985), (18525, 133.032), (9893, 128.856), (1022, 127.709), (19159, 127.676), (24240, 122.238)]\n",
      "02 Ġa: [(6991, 2498.207), (4576, 750.61), (37972, 440.728), (21072, 392.297), (18118, 392.204), (41425, 363.417), (10856, 356.684), (9893, 328.455), (46030, 327.483), (368, 313.049), (1687, 231.303), (26034, 222.958), (20111, 214.715), (28084, 212.877), (29357, 208.198), (34689, 208.154), (44319, 201.724), (37700, 191.561), (2000, 191.052), (17495, 184.728), (34312, 175.201), (2901, 166.421), (1022, 149.363), (5064, 136.864), (21909, 133.813), (31259, 133.523), (25752, 132.246), (5772, 126.684), (37084, 121.71), (13121, 120.893), (11502, 120.404), (31942, 117.785)]\n",
      "03 Ġmarathon: [(6991, 2492.298), (4576, 732.282), (37972, 606.466), (37303, 491.23), (1876, 365.049), (43326, 306.005), (10856, 282.913), (7747, 274.161), (34951, 267.048), (18118, 255.153), (5772, 226.147), (38575, 222.83), (37247, 220.12), (31259, 219.348), (13657, 185.737), (21072, 181.912), (16466, 170.245), (14277, 167.795), (29357, 163.408), (32949, 161.938), (9545, 158.672), (20398, 158.464), (1340, 154.294), (7981, 151.09), (30608, 143.962), (14881, 141.935), (14159, 139.476), (21049, 139.397), (25847, 135.033), (36057, 130.688), (9893, 130.491), (9061, 129.56)]\n",
      "04 ,: [(6991, 2221.066), (4576, 750.581), (10856, 487.256), (18118, 390.791), (37972, 388.959), (14055, 296.394), (5772, 295.494), (46030, 283.675), (37303, 269.127), (45536, 264.553), (34689, 213.958), (44319, 212.024), (14688, 195.584), (24885, 187.288), (33207, 177.398), (38575, 158.909), (20111, 150.219), (43326, 149.287), (31259, 144.619), (30608, 142.065), (1876, 140.771), (22999, 140.413), (30773, 138.464), (13646, 130.008), (25847, 127.361), (21172, 127.289), (34312, 120.016), (35073, 119.191), (43983, 118.283), (7747, 116.078), (32268, 113.908), (36565, 108.674)]\n",
      "05 Ġnot: [(6991, 2101.861), (236, 593.534), (37972, 588.61), (5772, 471.035), (4116, 457.448), (43326, 379.428), (7471, 340.895), (4576, 316.661), (10856, 295.357), (18118, 288.89), (23830, 262.071), (37303, 241.122), (1876, 218.101), (34951, 214.083), (13657, 213.357), (23120, 196.204), (43106, 194.883), (3644, 187.246), (38662, 186.901), (43357, 179.306), (45039, 175.274), (12138, 174.539), (36057, 172.005), (21721, 170.796), (3536, 170.229), (31259, 164.205), (8160, 163.351), (26034, 156.33), (14688, 152.007), (34689, 150.713), (39946, 150.483), (33, 149.136)]\n",
      "06 Ġa: [(6991, 2268.146), (37972, 545.4), (236, 441.811), (5772, 408.154), (34951, 350.502), (18118, 328.663), (4116, 306.027), (21072, 280.392), (16112, 277.382), (7471, 272.555), (4576, 263.996), (45039, 259.833), (13657, 238.577), (39946, 230.876), (10856, 228.026), (38662, 217.334), (12436, 209.273), (43357, 206.088), (20993, 188.328), (28084, 187.281), (43326, 174.178), (368, 164.499), (44082, 162.601), (26034, 160.789), (29357, 160.383), (15406, 159.363), (3644, 157.078), (23120, 150.387), (37303, 150.279), (8160, 143.84), (10329, 143.619), (34361, 141.749)]\n",
      "07 Ġsprint: [(6991, 2224.019), (4576, 730.915), (37972, 565.753), (34951, 541.954), (37303, 314.692), (39946, 295.484), (5772, 293.895), (45129, 258.864), (32328, 256.268), (18118, 253.785), (20993, 226.751), (29915, 212.715), (44178, 207.301), (31259, 203.929), (10856, 200.513), (45039, 191.902), (9061, 190.959), (29357, 181.17), (9545, 173.443), (24001, 168.89), (19273, 166.687), (23120, 166.514), (2122, 150.073), (3447, 144.7), (42983, 143.438), (33525, 138.148), (38575, 137.77), (43357, 136.245), (1962, 129.355), (34741, 128.204), (38662, 126.813), (25664, 124.32)]\n",
      "08 .: [(6991, 2193.079), (4576, 701.923), (10856, 557.996), (37972, 449.653), (46030, 359.246), (18118, 300.488), (45536, 285.899), (5772, 246.809), (31259, 235.165), (30773, 215.445), (30608, 214.249), (23391, 186.559), (14688, 180.132), (9545, 179.306), (37303, 177.197), (34689, 173.147), (29177, 167.623), (24646, 148.497), (43357, 136.558), (28869, 131.177), (43571, 128.215), (4922, 124.624), (2901, 118.825), (44319, 116.76), (13801, 113.521), (27970, 110.865), (32477, 108.311), (864, 104.125), (20111, 103.491), (7747, 101.681), (38662, 100.969), (33914, 98.695)]\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt = \"Life is a marathon, not a sprint.\"\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/gpt-oss-20b\")\n",
    "enc = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "enc = {k: v.to(model.device) for k, v in enc.items()}\n",
    "str_tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0])\n",
    "\n",
    "# Grab the transformer layers container and pick layer 15 (0-indexed)\n",
    "def get_layers_container(m):\n",
    "    for path in [\n",
    "        \"model.gpt_neox.layers\", \"gpt_neox.layers\", \"model.layers\",\n",
    "        \"model.transformer.h\", \"transformer.h\", \"model.decoder.layers\", \"decoder.layers\",\n",
    "    ]:\n",
    "        cur = m\n",
    "        ok = True\n",
    "        for p in path.split(\".\"):\n",
    "            if not hasattr(cur, p):\n",
    "                ok = False; break\n",
    "            cur = getattr(cur, p)\n",
    "        if ok and isinstance(cur, (torch.nn.ModuleList, list)):\n",
    "            return cur\n",
    "    raise RuntimeError(\"layers container not found\")\n",
    "\n",
    "layers = get_layers_container(model)\n",
    "target_layer = layers[15]\n",
    "\n",
    "# Capture activations at layer 15\n",
    "captures = {}\n",
    "def hook_fn(module, inputs, output):\n",
    "    hs = output[0] if isinstance(output, tuple) else output\n",
    "    captures[\"acts\"] = hs.detach()\n",
    "\n",
    "h = target_layer.register_forward_hook(hook_fn)\n",
    "with torch.no_grad():\n",
    "    _ = model(**enc)\n",
    "h.remove()\n",
    "\n",
    "# Pass through SAE and show active features for each token\n",
    "acts = captures[\"acts\"]\n",
    "B, T, H = acts.shape\n",
    "flat = acts.reshape(B * T, H).to(dtype=torch.float32, device=sae.decoder.weight.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    topk = sae.encode(flat)  # returns torch.return_types.topk with .values and .indices\n",
    "\n",
    "idx = topk.indices.reshape(B, T, -1)[0].cpu()\n",
    "val = topk.values.reshape(B, T, -1)[0].cpu()\n",
    "\n",
    "# Feature 1340 activates on the token 'marathon' in the context of \"marathon, not a sprint\"\n",
    "for t, tok in enumerate(str_tokens):\n",
    "    feats = list(zip(idx[t].tolist(), [round(v, 3) for v in val[t].tolist()]))\n",
    "    print(f\"{t:02d} {tok}: {feats}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
